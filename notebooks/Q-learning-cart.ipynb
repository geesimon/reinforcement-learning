{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep $Q$-learning\n",
    "\n",
    "In this notebook, we'll build a neural network that can learn to play games through reinforcement learning. More specifically, we'll use $Q$-learning to train an agent to play a game called [Cart-Pole](https://gym.openai.com/envs/CartPole-v0). In this game, a freely swinging pole is attached to a cart. The cart can move to the left and right, and the goal is to keep the pole upright as long as possible.\n",
    "\n",
    "![Cart-Pole](assets/cart-pole.jpg)\n",
    "\n",
    "We can simulate this game using [OpenAI Gym](https://github.com/openai/gym). First, let's check out how OpenAI Gym works. Then, we'll get into training an agent to play the Cart-Pole game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "Number of possible actions: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\geesi\\desktop\\dl\\gym\\gym\\__init__.py:22: UserWarning: DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.\n",
      "  warnings.warn('DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.')\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "# Create the Cart-Pole game environment\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "# Number of possible actions\n",
    "print('Number of possible actions:', env.action_space.n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We interact with the simulation through `env`.  You can see how many actions are possible from `env.action_space.n`, and to get a random action you can use `env.action_space.sample()`.  Passing in an action as an integer to `env.step` will generate the next step in the simulation.  This is general to all Gym games. \n",
    "\n",
    "In the Cart-Pole game, there are two possible actions, moving the cart left or right. So there are two actions we can take, encoded as 0 and 1.\n",
    "\n",
    "Run the code below to interact with the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "actions = [] # actions that the agent selects\n",
    "rewards = [] # obtained rewards\n",
    "state = env.reset()\n",
    "\n",
    "while True:\n",
    "    action = env.action_space.sample()  # choose a random action\n",
    "    state, reward, done, _ = env.step(action) \n",
    "    rewards.append(reward)\n",
    "    actions.append(action)\n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at the actions and rewards:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actions: [0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0]\n",
      "Rewards: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "print('Actions:', actions)\n",
    "print('Rewards:', rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The game resets after the pole has fallen past a certain angle. For each step while the game is running, it returns a reward of 1.0. The longer the game runs, the more reward we get. Then, our network's goal is to maximize the reward by keeping the pole vertical. It will do this by moving the cart to the left and the right.\n",
    "\n",
    "## $Q$-Network\n",
    "\n",
    "To keep track of the action values, we'll use a neural network that accepts a state $s$ as input.  The output will be $Q$-values for each available action $a$ (i.e., the output is **all** action values $Q(s,a)$ _corresponding to the input state $s$_).\n",
    "\n",
    "<img src=\"assets/q-network.png\" width=550px>\n",
    "\n",
    "For this Cart-Pole game, the state has four values: the position and velocity of the cart, and the position and velocity of the pole.  Thus, the neural network has **four inputs**, one for each value in the state, and **two outputs**, one for each possible action. \n",
    "\n",
    "As explored in the lesson, to get the training target, we'll first use the context provided by the state $s$ to choose an action $a$, then simulate the game using that action. This will get us the next state, $s'$, and the reward $r$. With that, we can calculate $\\hat{Q}(s,a) = r + \\gamma \\max_{a'}{Q(s', a')}$.  Then we update the weights by minimizing $(\\hat{Q}(s,a) - Q(s,a))^2$. \n",
    "\n",
    "Below is one implementation of the $Q$-network. It uses two fully connected layers with ReLU activations. Two seems to be good enough, three might be better. Feel free to try it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\geesi\\Anaconda3\\envs\\dlnd\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class QNetwork:\n",
    "    def __init__(self, learning_rate=0.01, state_size=4, \n",
    "                 action_size=2, hidden_size=10, \n",
    "                 name='QNetwork'):\n",
    "        # state inputs to the Q-network\n",
    "        with tf.variable_scope(name):\n",
    "            self.inputs_ = tf.placeholder(tf.float32, [None, state_size], name='inputs')\n",
    "            \n",
    "            # One hot encode the actions to later choose the Q-value for the action\n",
    "            self.actions_ = tf.placeholder(tf.int32, [None], name='actions')\n",
    "            one_hot_actions = tf.one_hot(self.actions_, action_size)\n",
    "            \n",
    "            # Target Q values for training\n",
    "            self.targetQs_ = tf.placeholder(tf.float32, [None], name='target')\n",
    "            \n",
    "            # ReLU hidden layers\n",
    "            self.fc1 = tf.contrib.layers.fully_connected(self.inputs_, hidden_size)\n",
    "            self.fc2 = tf.contrib.layers.fully_connected(self.fc1, hidden_size)\n",
    "\n",
    "            # Linear output layer\n",
    "            self.output = tf.contrib.layers.fully_connected(self.fc2, action_size, \n",
    "                                                            activation_fn=None)\n",
    "            \n",
    "            ### Train with loss (targetQ - Q)^2\n",
    "            # output has length 2, for two actions. This next line chooses\n",
    "            # one value from output (per row) according to the one-hot encoded actions.\n",
    "            self.Q = tf.reduce_sum(tf.multiply(self.output, one_hot_actions), axis=1)\n",
    "            \n",
    "            self.loss = tf.reduce_mean(tf.square(self.targetQs_ - self.Q))\n",
    "            self.opt = tf.train.AdamOptimizer(learning_rate).minimize(self.loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experience replay\n",
    "\n",
    "Reinforcement learning algorithms can have stability issues due to correlations between states. To reduce correlations when training, we can store the agent's experiences and later draw a random mini-batch of those experiences to train on. \n",
    "\n",
    "Here, we'll create a `Memory` object that will store our experiences, our transitions $<s, a, r, s'>$. This memory will have a maximum capacity, so we can keep newer experiences in memory while getting rid of older experiences. Then, we'll sample a random mini-batch of transitions $<s, a, r, s'>$ and train on those.\n",
    "\n",
    "Below, I've implemented a `Memory` object. If you're unfamiliar with `deque`, this is a double-ended queue. You can think of it like a tube open on both sides. You can put objects in either side of the tube. But if it's full, adding anything more will push an object out the other side. This is a great data structure to use for the memory buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "class Memory():\n",
    "    def __init__(self, max_size=1000):\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "    \n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "            \n",
    "    def sample(self, batch_size):\n",
    "        idx = np.random.choice(np.arange(len(self.buffer)), \n",
    "                               size=batch_size, \n",
    "                               replace=False)\n",
    "        return [self.buffer[ii] for ii in idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $Q$-Learning training algorithm\n",
    "\n",
    "We will use the below algorithm to train the network.  For this game, the goal is to keep the pole upright for 195 frames. So we can start a new episode once meeting that goal. The game ends if the pole tilts over too far, or if the cart moves too far the left or right. When a game ends, we'll start a new episode. Now, to train the agent:\n",
    "\n",
    "* Initialize the memory $D$\n",
    "* Initialize the action-value network $Q$ with random weights\n",
    "* **For** episode $\\leftarrow 1$ **to** $M$ **do**\n",
    "  * Observe $s_0$\n",
    "  * **For** $t \\leftarrow 0$ **to** $T-1$ **do**\n",
    "     * With probability $\\epsilon$ select a random action $a_t$, otherwise select $a_t = \\mathrm{argmax}_a Q(s_t,a)$\n",
    "     * Execute action $a_t$ in simulator and observe reward $r_{t+1}$ and new state $s_{t+1}$\n",
    "     * Store transition $<s_t, a_t, r_{t+1}, s_{t+1}>$ in memory $D$\n",
    "     * Sample random mini-batch from $D$: $<s_j, a_j, r_j, s'_j>$\n",
    "     * Set $\\hat{Q}_j = r_j$ if the episode ends at $j+1$, otherwise set $\\hat{Q}_j = r_j + \\gamma \\max_{a'}{Q(s'_j, a')}$\n",
    "     * Make a gradient descent step with loss $(\\hat{Q}_j - Q(s_j, a_j))^2$\n",
    "  * **endfor**\n",
    "* **endfor**\n",
    "\n",
    "You are welcome (and encouraged!) to take the time to extend this code to implement some of the improvements that we discussed in the lesson, to include fixed $Q$ targets, double DQNs, prioritized replay, and/or dueling networks.\n",
    "\n",
    "## Hyperparameters\n",
    "\n",
    "One of the more difficult aspects of reinforcement learning is the large number of hyperparameters. Not only are we tuning the network, but we're tuning the simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_episodes = 1000          # max number of episodes to learn from\n",
    "max_steps = 200                # max steps in an episode\n",
    "gamma = 0.99                   # future reward discount\n",
    "\n",
    "# Exploration parameters\n",
    "explore_start = 1.0            # exploration probability at start\n",
    "explore_stop = 0.01            # minimum exploration probability \n",
    "decay_rate = 0.0001            # exponential decay rate for exploration prob\n",
    "\n",
    "# Network parameters\n",
    "hidden_size = 64               # number of units in each Q-network hidden layer\n",
    "learning_rate = 0.0001         # Q-network learning rate\n",
    "\n",
    "# Memory parameters\n",
    "memory_size = 10000            # memory capacity\n",
    "batch_size = 20                # experience mini-batch size\n",
    "pretrain_length = batch_size   # number experiences to pretrain the memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "mainQN = QNetwork(name='main', hidden_size=hidden_size, learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Populate the experience memory\n",
    "\n",
    "Here we re-initialize the simulation and pre-populate the memory. The agent is taking random actions and storing the transitions in memory. This will help the agent with exploring the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize the simulation\n",
    "env.reset()\n",
    "# Take one random step to get the pole and cart moving\n",
    "state, reward, done, _ = env.step(env.action_space.sample())\n",
    "\n",
    "memory = Memory(max_size=memory_size)\n",
    "\n",
    "# Make a bunch of random actions and store the experiences\n",
    "for ii in range(pretrain_length):\n",
    "\n",
    "    # Make a random action\n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "    if done:\n",
    "        # The simulation fails so no next state\n",
    "        next_state = np.zeros(state.shape)\n",
    "        # Add experience to memory\n",
    "        memory.add((state, action, reward, next_state))\n",
    "        \n",
    "        # Start new episode\n",
    "        env.reset()\n",
    "        # Take one random step to get the pole and cart moving\n",
    "        state, reward, done, _ = env.step(env.action_space.sample())\n",
    "    else:\n",
    "        # Add experience to memory\n",
    "        memory.add((state, action, reward, next_state))\n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Below we'll train our agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1 Total reward: 10.0 Training loss: 1.1227 Explore P: 0.9990\n",
      "Episode: 2 Total reward: 12.0 Training loss: 1.1467 Explore P: 0.9978\n",
      "Episode: 3 Total reward: 24.0 Training loss: 1.0843 Explore P: 0.9955\n",
      "Episode: 4 Total reward: 10.0 Training loss: 1.0980 Explore P: 0.9945\n",
      "Episode: 5 Total reward: 25.0 Training loss: 1.0772 Explore P: 0.9920\n",
      "Episode: 6 Total reward: 9.0 Training loss: 1.0365 Explore P: 0.9911\n",
      "Episode: 7 Total reward: 14.0 Training loss: 1.0500 Explore P: 0.9898\n",
      "Episode: 8 Total reward: 15.0 Training loss: 1.1376 Explore P: 0.9883\n",
      "Episode: 9 Total reward: 19.0 Training loss: 1.1872 Explore P: 0.9864\n",
      "Episode: 10 Total reward: 17.0 Training loss: 1.1649 Explore P: 0.9848\n",
      "Episode: 11 Total reward: 17.0 Training loss: 1.1929 Explore P: 0.9831\n",
      "Episode: 12 Total reward: 20.0 Training loss: 1.1554 Explore P: 0.9812\n",
      "Episode: 13 Total reward: 25.0 Training loss: 1.1993 Explore P: 0.9787\n",
      "Episode: 14 Total reward: 20.0 Training loss: 1.2089 Explore P: 0.9768\n",
      "Episode: 15 Total reward: 21.0 Training loss: 1.3449 Explore P: 0.9748\n",
      "Episode: 16 Total reward: 9.0 Training loss: 1.4643 Explore P: 0.9739\n",
      "Episode: 17 Total reward: 9.0 Training loss: 1.4548 Explore P: 0.9730\n",
      "Episode: 18 Total reward: 15.0 Training loss: 1.6376 Explore P: 0.9716\n",
      "Episode: 19 Total reward: 30.0 Training loss: 1.5428 Explore P: 0.9687\n",
      "Episode: 20 Total reward: 25.0 Training loss: 1.7725 Explore P: 0.9663\n",
      "Episode: 21 Total reward: 17.0 Training loss: 1.8817 Explore P: 0.9647\n",
      "Episode: 22 Total reward: 22.0 Training loss: 2.0663 Explore P: 0.9626\n",
      "Episode: 23 Total reward: 32.0 Training loss: 2.0496 Explore P: 0.9596\n",
      "Episode: 24 Total reward: 12.0 Training loss: 3.0665 Explore P: 0.9584\n",
      "Episode: 25 Total reward: 25.0 Training loss: 2.6195 Explore P: 0.9561\n",
      "Episode: 26 Total reward: 42.0 Training loss: 4.2208 Explore P: 0.9521\n",
      "Episode: 27 Total reward: 10.0 Training loss: 2.5373 Explore P: 0.9512\n",
      "Episode: 28 Total reward: 14.0 Training loss: 5.6786 Explore P: 0.9498\n",
      "Episode: 29 Total reward: 11.0 Training loss: 6.8912 Explore P: 0.9488\n",
      "Episode: 30 Total reward: 14.0 Training loss: 8.0416 Explore P: 0.9475\n",
      "Episode: 31 Total reward: 9.0 Training loss: 3.4553 Explore P: 0.9466\n",
      "Episode: 32 Total reward: 11.0 Training loss: 3.4604 Explore P: 0.9456\n",
      "Episode: 33 Total reward: 15.0 Training loss: 8.9810 Explore P: 0.9442\n",
      "Episode: 34 Total reward: 14.0 Training loss: 5.1645 Explore P: 0.9429\n",
      "Episode: 35 Total reward: 13.0 Training loss: 10.7482 Explore P: 0.9417\n",
      "Episode: 36 Total reward: 14.0 Training loss: 4.1927 Explore P: 0.9404\n",
      "Episode: 37 Total reward: 20.0 Training loss: 3.6349 Explore P: 0.9385\n",
      "Episode: 38 Total reward: 13.0 Training loss: 13.6661 Explore P: 0.9373\n",
      "Episode: 39 Total reward: 15.0 Training loss: 8.0774 Explore P: 0.9359\n",
      "Episode: 40 Total reward: 13.0 Training loss: 6.1549 Explore P: 0.9347\n",
      "Episode: 41 Total reward: 10.0 Training loss: 20.2403 Explore P: 0.9338\n",
      "Episode: 42 Total reward: 10.0 Training loss: 4.3489 Explore P: 0.9329\n",
      "Episode: 43 Total reward: 16.0 Training loss: 11.3000 Explore P: 0.9314\n",
      "Episode: 44 Total reward: 13.0 Training loss: 13.7614 Explore P: 0.9302\n",
      "Episode: 45 Total reward: 22.0 Training loss: 4.2994 Explore P: 0.9282\n",
      "Episode: 46 Total reward: 22.0 Training loss: 10.1747 Explore P: 0.9262\n",
      "Episode: 47 Total reward: 12.0 Training loss: 28.4045 Explore P: 0.9251\n",
      "Episode: 48 Total reward: 14.0 Training loss: 7.8927 Explore P: 0.9238\n",
      "Episode: 49 Total reward: 9.0 Training loss: 37.0112 Explore P: 0.9230\n",
      "Episode: 50 Total reward: 13.0 Training loss: 31.0381 Explore P: 0.9218\n",
      "Episode: 51 Total reward: 27.0 Training loss: 30.8874 Explore P: 0.9193\n",
      "Episode: 52 Total reward: 18.0 Training loss: 12.1170 Explore P: 0.9177\n",
      "Episode: 53 Total reward: 32.0 Training loss: 7.7995 Explore P: 0.9148\n",
      "Episode: 54 Total reward: 13.0 Training loss: 16.4691 Explore P: 0.9136\n",
      "Episode: 55 Total reward: 15.0 Training loss: 31.8817 Explore P: 0.9123\n",
      "Episode: 56 Total reward: 15.0 Training loss: 24.7354 Explore P: 0.9109\n",
      "Episode: 57 Total reward: 9.0 Training loss: 55.3899 Explore P: 0.9101\n",
      "Episode: 58 Total reward: 17.0 Training loss: 6.8580 Explore P: 0.9086\n",
      "Episode: 59 Total reward: 12.0 Training loss: 28.8762 Explore P: 0.9075\n",
      "Episode: 60 Total reward: 42.0 Training loss: 8.9303 Explore P: 0.9037\n",
      "Episode: 61 Total reward: 13.0 Training loss: 7.7737 Explore P: 0.9026\n",
      "Episode: 62 Total reward: 27.0 Training loss: 29.2619 Explore P: 0.9002\n",
      "Episode: 63 Total reward: 11.0 Training loss: 7.5607 Explore P: 0.8992\n",
      "Episode: 64 Total reward: 16.0 Training loss: 9.4440 Explore P: 0.8978\n",
      "Episode: 65 Total reward: 21.0 Training loss: 50.2477 Explore P: 0.8959\n",
      "Episode: 66 Total reward: 29.0 Training loss: 60.6622 Explore P: 0.8933\n",
      "Episode: 67 Total reward: 11.0 Training loss: 68.6176 Explore P: 0.8924\n",
      "Episode: 68 Total reward: 30.0 Training loss: 13.5446 Explore P: 0.8897\n",
      "Episode: 69 Total reward: 18.0 Training loss: 77.2922 Explore P: 0.8881\n",
      "Episode: 70 Total reward: 25.0 Training loss: 93.8907 Explore P: 0.8859\n",
      "Episode: 71 Total reward: 12.0 Training loss: 106.2620 Explore P: 0.8849\n",
      "Episode: 72 Total reward: 9.0 Training loss: 54.7772 Explore P: 0.8841\n",
      "Episode: 73 Total reward: 10.0 Training loss: 9.1968 Explore P: 0.8832\n",
      "Episode: 74 Total reward: 23.0 Training loss: 48.4893 Explore P: 0.8812\n",
      "Episode: 75 Total reward: 13.0 Training loss: 51.3162 Explore P: 0.8801\n",
      "Episode: 76 Total reward: 32.0 Training loss: 32.7654 Explore P: 0.8773\n",
      "Episode: 77 Total reward: 16.0 Training loss: 63.5354 Explore P: 0.8759\n",
      "Episode: 78 Total reward: 16.0 Training loss: 70.0406 Explore P: 0.8745\n",
      "Episode: 79 Total reward: 16.0 Training loss: 10.5510 Explore P: 0.8732\n",
      "Episode: 80 Total reward: 22.0 Training loss: 43.5690 Explore P: 0.8713\n",
      "Episode: 81 Total reward: 8.0 Training loss: 53.2693 Explore P: 0.8706\n",
      "Episode: 82 Total reward: 12.0 Training loss: 66.6576 Explore P: 0.8695\n",
      "Episode: 83 Total reward: 43.0 Training loss: 26.1398 Explore P: 0.8659\n",
      "Episode: 84 Total reward: 18.0 Training loss: 8.9366 Explore P: 0.8643\n",
      "Episode: 85 Total reward: 10.0 Training loss: 29.9237 Explore P: 0.8635\n",
      "Episode: 86 Total reward: 27.0 Training loss: 51.8360 Explore P: 0.8612\n",
      "Episode: 87 Total reward: 31.0 Training loss: 101.3132 Explore P: 0.8585\n",
      "Episode: 88 Total reward: 14.0 Training loss: 36.0114 Explore P: 0.8573\n",
      "Episode: 89 Total reward: 10.0 Training loss: 8.6057 Explore P: 0.8565\n",
      "Episode: 90 Total reward: 8.0 Training loss: 50.5653 Explore P: 0.8558\n",
      "Episode: 91 Total reward: 34.0 Training loss: 8.7847 Explore P: 0.8529\n",
      "Episode: 92 Total reward: 11.0 Training loss: 13.6556 Explore P: 0.8520\n",
      "Episode: 93 Total reward: 10.0 Training loss: 163.9562 Explore P: 0.8512\n",
      "Episode: 94 Total reward: 12.0 Training loss: 7.8265 Explore P: 0.8502\n",
      "Episode: 95 Total reward: 13.0 Training loss: 8.9952 Explore P: 0.8491\n",
      "Episode: 96 Total reward: 24.0 Training loss: 193.7463 Explore P: 0.8471\n",
      "Episode: 97 Total reward: 9.0 Training loss: 98.3284 Explore P: 0.8463\n",
      "Episode: 98 Total reward: 8.0 Training loss: 6.8173 Explore P: 0.8456\n",
      "Episode: 99 Total reward: 24.0 Training loss: 8.3212 Explore P: 0.8436\n",
      "Episode: 100 Total reward: 27.0 Training loss: 87.0633 Explore P: 0.8414\n",
      "Episode: 101 Total reward: 14.0 Training loss: 74.1523 Explore P: 0.8402\n",
      "Episode: 102 Total reward: 9.0 Training loss: 47.0876 Explore P: 0.8395\n",
      "Episode: 103 Total reward: 12.0 Training loss: 69.2448 Explore P: 0.8385\n",
      "Episode: 104 Total reward: 13.0 Training loss: 80.2878 Explore P: 0.8374\n",
      "Episode: 105 Total reward: 14.0 Training loss: 69.0242 Explore P: 0.8363\n",
      "Episode: 106 Total reward: 13.0 Training loss: 44.1944 Explore P: 0.8352\n",
      "Episode: 107 Total reward: 33.0 Training loss: 7.2274 Explore P: 0.8325\n",
      "Episode: 108 Total reward: 12.0 Training loss: 6.9702 Explore P: 0.8315\n",
      "Episode: 109 Total reward: 13.0 Training loss: 90.7509 Explore P: 0.8304\n",
      "Episode: 110 Total reward: 12.0 Training loss: 64.9322 Explore P: 0.8294\n",
      "Episode: 111 Total reward: 13.0 Training loss: 9.6443 Explore P: 0.8284\n",
      "Episode: 112 Total reward: 12.0 Training loss: 6.0475 Explore P: 0.8274\n",
      "Episode: 113 Total reward: 14.0 Training loss: 35.5343 Explore P: 0.8262\n",
      "Episode: 114 Total reward: 22.0 Training loss: 66.3317 Explore P: 0.8244\n",
      "Episode: 115 Total reward: 23.0 Training loss: 94.0949 Explore P: 0.8226\n",
      "Episode: 116 Total reward: 11.0 Training loss: 77.5243 Explore P: 0.8217\n",
      "Episode: 117 Total reward: 13.0 Training loss: 7.6582 Explore P: 0.8206\n",
      "Episode: 118 Total reward: 9.0 Training loss: 7.4721 Explore P: 0.8199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 119 Total reward: 54.0 Training loss: 6.5895 Explore P: 0.8155\n",
      "Episode: 120 Total reward: 14.0 Training loss: 127.2907 Explore P: 0.8144\n",
      "Episode: 121 Total reward: 14.0 Training loss: 109.3961 Explore P: 0.8133\n",
      "Episode: 122 Total reward: 11.0 Training loss: 7.1651 Explore P: 0.8124\n",
      "Episode: 123 Total reward: 11.0 Training loss: 242.5113 Explore P: 0.8115\n",
      "Episode: 124 Total reward: 13.0 Training loss: 146.3520 Explore P: 0.8105\n",
      "Episode: 125 Total reward: 12.0 Training loss: 35.4247 Explore P: 0.8095\n",
      "Episode: 126 Total reward: 17.0 Training loss: 78.5297 Explore P: 0.8082\n",
      "Episode: 127 Total reward: 23.0 Training loss: 96.3533 Explore P: 0.8063\n",
      "Episode: 128 Total reward: 21.0 Training loss: 95.0700 Explore P: 0.8047\n",
      "Episode: 129 Total reward: 9.0 Training loss: 149.5548 Explore P: 0.8039\n",
      "Episode: 130 Total reward: 19.0 Training loss: 5.3272 Explore P: 0.8024\n",
      "Episode: 131 Total reward: 21.0 Training loss: 91.9849 Explore P: 0.8008\n",
      "Episode: 132 Total reward: 43.0 Training loss: 34.1547 Explore P: 0.7974\n",
      "Episode: 133 Total reward: 18.0 Training loss: 5.5732 Explore P: 0.7960\n",
      "Episode: 134 Total reward: 22.0 Training loss: 98.6836 Explore P: 0.7942\n",
      "Episode: 135 Total reward: 14.0 Training loss: 173.5960 Explore P: 0.7931\n",
      "Episode: 136 Total reward: 13.0 Training loss: 4.4933 Explore P: 0.7921\n",
      "Episode: 137 Total reward: 28.0 Training loss: 129.8549 Explore P: 0.7899\n",
      "Episode: 138 Total reward: 11.0 Training loss: 114.7188 Explore P: 0.7891\n",
      "Episode: 139 Total reward: 10.0 Training loss: 173.5973 Explore P: 0.7883\n",
      "Episode: 140 Total reward: 10.0 Training loss: 4.8010 Explore P: 0.7875\n",
      "Episode: 141 Total reward: 19.0 Training loss: 3.6231 Explore P: 0.7860\n",
      "Episode: 142 Total reward: 15.0 Training loss: 4.5027 Explore P: 0.7849\n",
      "Episode: 143 Total reward: 35.0 Training loss: 75.8054 Explore P: 0.7822\n",
      "Episode: 144 Total reward: 9.0 Training loss: 4.1618 Explore P: 0.7815\n",
      "Episode: 145 Total reward: 19.0 Training loss: 3.4118 Explore P: 0.7800\n",
      "Episode: 146 Total reward: 30.0 Training loss: 54.1642 Explore P: 0.7777\n",
      "Episode: 147 Total reward: 15.0 Training loss: 3.2060 Explore P: 0.7766\n",
      "Episode: 148 Total reward: 13.0 Training loss: 63.7241 Explore P: 0.7756\n",
      "Episode: 149 Total reward: 21.0 Training loss: 35.1914 Explore P: 0.7740\n",
      "Episode: 150 Total reward: 16.0 Training loss: 69.3686 Explore P: 0.7727\n",
      "Episode: 151 Total reward: 18.0 Training loss: 51.3994 Explore P: 0.7714\n",
      "Episode: 152 Total reward: 19.0 Training loss: 49.1118 Explore P: 0.7699\n",
      "Episode: 153 Total reward: 15.0 Training loss: 36.9551 Explore P: 0.7688\n",
      "Episode: 154 Total reward: 30.0 Training loss: 3.9542 Explore P: 0.7665\n",
      "Episode: 155 Total reward: 10.0 Training loss: 37.4166 Explore P: 0.7657\n",
      "Episode: 156 Total reward: 8.0 Training loss: 97.7709 Explore P: 0.7651\n",
      "Episode: 157 Total reward: 15.0 Training loss: 75.4163 Explore P: 0.7640\n",
      "Episode: 158 Total reward: 7.0 Training loss: 101.7808 Explore P: 0.7635\n",
      "Episode: 159 Total reward: 9.0 Training loss: 4.9156 Explore P: 0.7628\n",
      "Episode: 160 Total reward: 55.0 Training loss: 3.8960 Explore P: 0.7587\n",
      "Episode: 161 Total reward: 22.0 Training loss: 67.0365 Explore P: 0.7570\n",
      "Episode: 162 Total reward: 19.0 Training loss: 54.9167 Explore P: 0.7556\n",
      "Episode: 163 Total reward: 11.0 Training loss: 73.2395 Explore P: 0.7548\n",
      "Episode: 164 Total reward: 31.0 Training loss: 54.9696 Explore P: 0.7525\n",
      "Episode: 165 Total reward: 11.0 Training loss: 257.8288 Explore P: 0.7517\n",
      "Episode: 166 Total reward: 16.0 Training loss: 3.4343 Explore P: 0.7505\n",
      "Episode: 167 Total reward: 9.0 Training loss: 86.6386 Explore P: 0.7498\n",
      "Episode: 168 Total reward: 21.0 Training loss: 36.3723 Explore P: 0.7483\n",
      "Episode: 169 Total reward: 31.0 Training loss: 82.6934 Explore P: 0.7460\n",
      "Episode: 170 Total reward: 16.0 Training loss: 37.9454 Explore P: 0.7448\n",
      "Episode: 171 Total reward: 27.0 Training loss: 80.7832 Explore P: 0.7428\n",
      "Episode: 172 Total reward: 17.0 Training loss: 108.3497 Explore P: 0.7416\n",
      "Episode: 173 Total reward: 14.0 Training loss: 40.0258 Explore P: 0.7406\n",
      "Episode: 174 Total reward: 35.0 Training loss: 92.2698 Explore P: 0.7380\n",
      "Episode: 175 Total reward: 11.0 Training loss: 3.3195 Explore P: 0.7372\n",
      "Episode: 176 Total reward: 9.0 Training loss: 81.9457 Explore P: 0.7365\n",
      "Episode: 177 Total reward: 16.0 Training loss: 41.8301 Explore P: 0.7354\n",
      "Episode: 178 Total reward: 40.0 Training loss: 2.0680 Explore P: 0.7325\n",
      "Episode: 179 Total reward: 28.0 Training loss: 37.8488 Explore P: 0.7305\n",
      "Episode: 180 Total reward: 11.0 Training loss: 45.7663 Explore P: 0.7297\n",
      "Episode: 181 Total reward: 12.0 Training loss: 2.1079 Explore P: 0.7288\n",
      "Episode: 182 Total reward: 14.0 Training loss: 130.4673 Explore P: 0.7278\n",
      "Episode: 183 Total reward: 12.0 Training loss: 2.1632 Explore P: 0.7269\n",
      "Episode: 184 Total reward: 41.0 Training loss: 83.4625 Explore P: 0.7240\n",
      "Episode: 185 Total reward: 14.0 Training loss: 1.5544 Explore P: 0.7230\n",
      "Episode: 186 Total reward: 36.0 Training loss: 118.6952 Explore P: 0.7205\n",
      "Episode: 187 Total reward: 9.0 Training loss: 178.7782 Explore P: 0.7198\n",
      "Episode: 188 Total reward: 50.0 Training loss: 41.5457 Explore P: 0.7163\n",
      "Episode: 189 Total reward: 13.0 Training loss: 86.6804 Explore P: 0.7154\n",
      "Episode: 190 Total reward: 30.0 Training loss: 1.3358 Explore P: 0.7132\n",
      "Episode: 191 Total reward: 13.0 Training loss: 1.3755 Explore P: 0.7123\n",
      "Episode: 192 Total reward: 28.0 Training loss: 83.0767 Explore P: 0.7104\n",
      "Episode: 193 Total reward: 13.0 Training loss: 1.3725 Explore P: 0.7095\n",
      "Episode: 194 Total reward: 9.0 Training loss: 65.9627 Explore P: 0.7088\n",
      "Episode: 195 Total reward: 18.0 Training loss: 35.2861 Explore P: 0.7076\n",
      "Episode: 196 Total reward: 14.0 Training loss: 1.1043 Explore P: 0.7066\n",
      "Episode: 197 Total reward: 14.0 Training loss: 27.0874 Explore P: 0.7056\n",
      "Episode: 198 Total reward: 23.0 Training loss: 30.1622 Explore P: 0.7040\n",
      "Episode: 199 Total reward: 11.0 Training loss: 1.0642 Explore P: 0.7033\n",
      "Episode: 200 Total reward: 18.0 Training loss: 33.2927 Explore P: 0.7020\n",
      "Episode: 201 Total reward: 11.0 Training loss: 1.2567 Explore P: 0.7013\n",
      "Episode: 202 Total reward: 15.0 Training loss: 0.9705 Explore P: 0.7002\n",
      "Episode: 203 Total reward: 30.0 Training loss: 38.5072 Explore P: 0.6981\n",
      "Episode: 204 Total reward: 30.0 Training loss: 24.7798 Explore P: 0.6961\n",
      "Episode: 205 Total reward: 23.0 Training loss: 0.5737 Explore P: 0.6945\n",
      "Episode: 206 Total reward: 16.0 Training loss: 24.7253 Explore P: 0.6934\n",
      "Episode: 207 Total reward: 12.0 Training loss: 58.6176 Explore P: 0.6926\n",
      "Episode: 208 Total reward: 14.0 Training loss: 33.8583 Explore P: 0.6916\n",
      "Episode: 209 Total reward: 21.0 Training loss: 67.5200 Explore P: 0.6902\n",
      "Episode: 210 Total reward: 9.0 Training loss: 32.0201 Explore P: 0.6896\n",
      "Episode: 211 Total reward: 7.0 Training loss: 0.7973 Explore P: 0.6891\n",
      "Episode: 212 Total reward: 10.0 Training loss: 23.1247 Explore P: 0.6884\n",
      "Episode: 213 Total reward: 23.0 Training loss: 0.5944 Explore P: 0.6869\n",
      "Episode: 214 Total reward: 8.0 Training loss: 48.7810 Explore P: 0.6863\n",
      "Episode: 215 Total reward: 22.0 Training loss: 23.2149 Explore P: 0.6849\n",
      "Episode: 216 Total reward: 10.0 Training loss: 21.0932 Explore P: 0.6842\n",
      "Episode: 217 Total reward: 12.0 Training loss: 71.8600 Explore P: 0.6834\n",
      "Episode: 218 Total reward: 22.0 Training loss: 41.8132 Explore P: 0.6819\n",
      "Episode: 219 Total reward: 33.0 Training loss: 19.7495 Explore P: 0.6797\n",
      "Episode: 220 Total reward: 16.0 Training loss: 35.0469 Explore P: 0.6786\n",
      "Episode: 221 Total reward: 26.0 Training loss: 39.6193 Explore P: 0.6769\n",
      "Episode: 222 Total reward: 13.0 Training loss: 0.7038 Explore P: 0.6760\n",
      "Episode: 223 Total reward: 19.0 Training loss: 44.6947 Explore P: 0.6747\n",
      "Episode: 224 Total reward: 40.0 Training loss: 1.1956 Explore P: 0.6721\n",
      "Episode: 225 Total reward: 76.0 Training loss: 15.7851 Explore P: 0.6671\n",
      "Episode: 226 Total reward: 14.0 Training loss: 0.6189 Explore P: 0.6662\n",
      "Episode: 227 Total reward: 28.0 Training loss: 18.2283 Explore P: 0.6643\n",
      "Episode: 228 Total reward: 11.0 Training loss: 0.5859 Explore P: 0.6636\n",
      "Episode: 229 Total reward: 18.0 Training loss: 57.6204 Explore P: 0.6624\n",
      "Episode: 230 Total reward: 21.0 Training loss: 1.9501 Explore P: 0.6611\n",
      "Episode: 231 Total reward: 25.0 Training loss: 48.6900 Explore P: 0.6594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 232 Total reward: 24.0 Training loss: 0.6241 Explore P: 0.6579\n",
      "Episode: 233 Total reward: 18.0 Training loss: 0.7078 Explore P: 0.6567\n",
      "Episode: 234 Total reward: 14.0 Training loss: 39.5625 Explore P: 0.6558\n",
      "Episode: 235 Total reward: 34.0 Training loss: 14.3520 Explore P: 0.6536\n",
      "Episode: 236 Total reward: 25.0 Training loss: 30.1478 Explore P: 0.6520\n",
      "Episode: 237 Total reward: 25.0 Training loss: 12.8862 Explore P: 0.6504\n",
      "Episode: 238 Total reward: 29.0 Training loss: 1.0210 Explore P: 0.6486\n",
      "Episode: 239 Total reward: 28.0 Training loss: 13.2540 Explore P: 0.6468\n",
      "Episode: 240 Total reward: 33.0 Training loss: 23.5598 Explore P: 0.6447\n",
      "Episode: 241 Total reward: 86.0 Training loss: 0.9288 Explore P: 0.6392\n",
      "Episode: 242 Total reward: 49.0 Training loss: 0.9981 Explore P: 0.6362\n",
      "Episode: 243 Total reward: 22.0 Training loss: 27.6743 Explore P: 0.6348\n",
      "Episode: 244 Total reward: 60.0 Training loss: 23.6105 Explore P: 0.6310\n",
      "Episode: 245 Total reward: 82.0 Training loss: 34.2564 Explore P: 0.6260\n",
      "Episode: 246 Total reward: 28.0 Training loss: 13.2466 Explore P: 0.6243\n",
      "Episode: 247 Total reward: 22.0 Training loss: 25.5072 Explore P: 0.6229\n",
      "Episode: 248 Total reward: 49.0 Training loss: 23.2494 Explore P: 0.6199\n",
      "Episode: 249 Total reward: 24.0 Training loss: 55.0825 Explore P: 0.6184\n",
      "Episode: 250 Total reward: 9.0 Training loss: 33.6019 Explore P: 0.6179\n",
      "Episode: 251 Total reward: 29.0 Training loss: 1.0866 Explore P: 0.6161\n",
      "Episode: 252 Total reward: 61.0 Training loss: 0.9849 Explore P: 0.6125\n",
      "Episode: 253 Total reward: 15.0 Training loss: 20.9833 Explore P: 0.6115\n",
      "Episode: 254 Total reward: 23.0 Training loss: 45.8649 Explore P: 0.6102\n",
      "Episode: 255 Total reward: 38.0 Training loss: 12.7290 Explore P: 0.6079\n",
      "Episode: 256 Total reward: 49.0 Training loss: 28.2575 Explore P: 0.6050\n",
      "Episode: 257 Total reward: 53.0 Training loss: 21.5821 Explore P: 0.6018\n",
      "Episode: 258 Total reward: 66.0 Training loss: 0.8003 Explore P: 0.5979\n",
      "Episode: 259 Total reward: 94.0 Training loss: 1.5252 Explore P: 0.5924\n",
      "Episode: 260 Total reward: 34.0 Training loss: 10.6754 Explore P: 0.5905\n",
      "Episode: 261 Total reward: 29.0 Training loss: 24.8733 Explore P: 0.5888\n",
      "Episode: 262 Total reward: 109.0 Training loss: 12.4529 Explore P: 0.5825\n",
      "Episode: 263 Total reward: 48.0 Training loss: 36.0805 Explore P: 0.5798\n",
      "Episode: 264 Total reward: 34.0 Training loss: 1.4197 Explore P: 0.5778\n",
      "Episode: 265 Total reward: 25.0 Training loss: 22.4301 Explore P: 0.5764\n",
      "Episode: 266 Total reward: 18.0 Training loss: 1.1637 Explore P: 0.5754\n",
      "Episode: 267 Total reward: 24.0 Training loss: 1.1632 Explore P: 0.5740\n",
      "Episode: 268 Total reward: 22.0 Training loss: 0.7154 Explore P: 0.5728\n",
      "Episode: 269 Total reward: 22.0 Training loss: 0.8966 Explore P: 0.5716\n",
      "Episode: 270 Total reward: 38.0 Training loss: 1.0840 Explore P: 0.5694\n",
      "Episode: 271 Total reward: 36.0 Training loss: 1.0541 Explore P: 0.5674\n",
      "Episode: 272 Total reward: 53.0 Training loss: 15.7875 Explore P: 0.5645\n",
      "Episode: 273 Total reward: 64.0 Training loss: 30.1400 Explore P: 0.5609\n",
      "Episode: 274 Total reward: 43.0 Training loss: 62.0434 Explore P: 0.5586\n",
      "Episode: 275 Total reward: 25.0 Training loss: 0.9058 Explore P: 0.5572\n",
      "Episode: 276 Total reward: 26.0 Training loss: 10.5364 Explore P: 0.5558\n",
      "Episode: 277 Total reward: 58.0 Training loss: 22.6567 Explore P: 0.5526\n",
      "Episode: 278 Total reward: 21.0 Training loss: 1.2435 Explore P: 0.5515\n",
      "Episode: 279 Total reward: 25.0 Training loss: 24.6079 Explore P: 0.5501\n",
      "Episode: 280 Total reward: 27.0 Training loss: 21.9574 Explore P: 0.5487\n",
      "Episode: 281 Total reward: 79.0 Training loss: 23.0633 Explore P: 0.5444\n",
      "Episode: 282 Total reward: 68.0 Training loss: 1.0605 Explore P: 0.5408\n",
      "Episode: 283 Total reward: 71.0 Training loss: 1.2469 Explore P: 0.5371\n",
      "Episode: 284 Total reward: 65.0 Training loss: 0.9320 Explore P: 0.5336\n",
      "Episode: 285 Total reward: 74.0 Training loss: 1.3720 Explore P: 0.5298\n",
      "Episode: 286 Total reward: 54.0 Training loss: 14.4708 Explore P: 0.5270\n",
      "Episode: 287 Total reward: 32.0 Training loss: 1.2381 Explore P: 0.5253\n",
      "Episode: 288 Total reward: 46.0 Training loss: 0.6592 Explore P: 0.5230\n",
      "Episode: 289 Total reward: 74.0 Training loss: 1.7769 Explore P: 0.5192\n",
      "Episode: 290 Total reward: 44.0 Training loss: 32.4696 Explore P: 0.5169\n",
      "Episode: 291 Total reward: 25.0 Training loss: 14.8633 Explore P: 0.5157\n",
      "Episode: 292 Total reward: 13.0 Training loss: 1.0118 Explore P: 0.5150\n",
      "Episode: 293 Total reward: 44.0 Training loss: 0.9561 Explore P: 0.5128\n",
      "Episode: 294 Total reward: 30.0 Training loss: 1.7154 Explore P: 0.5113\n",
      "Episode: 295 Total reward: 39.0 Training loss: 29.2532 Explore P: 0.5093\n",
      "Episode: 296 Total reward: 23.0 Training loss: 19.9646 Explore P: 0.5082\n",
      "Episode: 297 Total reward: 24.0 Training loss: 27.1750 Explore P: 0.5070\n",
      "Episode: 298 Total reward: 27.0 Training loss: 1.5716 Explore P: 0.5057\n",
      "Episode: 299 Total reward: 34.0 Training loss: 1.2459 Explore P: 0.5040\n",
      "Episode: 300 Total reward: 20.0 Training loss: 1.2224 Explore P: 0.5030\n",
      "Episode: 301 Total reward: 18.0 Training loss: 35.1977 Explore P: 0.5021\n",
      "Episode: 302 Total reward: 32.0 Training loss: 17.1597 Explore P: 0.5005\n",
      "Episode: 303 Total reward: 35.0 Training loss: 86.5279 Explore P: 0.4988\n",
      "Episode: 304 Total reward: 37.0 Training loss: 1.1703 Explore P: 0.4970\n",
      "Episode: 305 Total reward: 28.0 Training loss: 35.3603 Explore P: 0.4957\n",
      "Episode: 306 Total reward: 47.0 Training loss: 20.1489 Explore P: 0.4934\n",
      "Episode: 307 Total reward: 137.0 Training loss: 26.4983 Explore P: 0.4868\n",
      "Episode: 308 Total reward: 38.0 Training loss: 0.7045 Explore P: 0.4850\n",
      "Episode: 309 Total reward: 36.0 Training loss: 56.5841 Explore P: 0.4833\n",
      "Episode: 310 Total reward: 32.0 Training loss: 40.1027 Explore P: 0.4818\n",
      "Episode: 311 Total reward: 39.0 Training loss: 24.1062 Explore P: 0.4799\n",
      "Episode: 312 Total reward: 35.0 Training loss: 2.3946 Explore P: 0.4783\n",
      "Episode: 313 Total reward: 10.0 Training loss: 33.2524 Explore P: 0.4778\n",
      "Episode: 314 Total reward: 80.0 Training loss: 1.4105 Explore P: 0.4741\n",
      "Episode: 315 Total reward: 55.0 Training loss: 33.7826 Explore P: 0.4716\n",
      "Episode: 316 Total reward: 22.0 Training loss: 41.5344 Explore P: 0.4705\n",
      "Episode: 317 Total reward: 21.0 Training loss: 1.1198 Explore P: 0.4696\n",
      "Episode: 318 Total reward: 49.0 Training loss: 23.5868 Explore P: 0.4673\n",
      "Episode: 319 Total reward: 56.0 Training loss: 1.7093 Explore P: 0.4648\n",
      "Episode: 320 Total reward: 21.0 Training loss: 24.0639 Explore P: 0.4638\n",
      "Episode: 321 Total reward: 75.0 Training loss: 1.2388 Explore P: 0.4604\n",
      "Episode: 322 Total reward: 113.0 Training loss: 0.9928 Explore P: 0.4554\n",
      "Episode: 323 Total reward: 86.0 Training loss: 25.7672 Explore P: 0.4516\n",
      "Episode: 324 Total reward: 43.0 Training loss: 64.5673 Explore P: 0.4497\n",
      "Episode: 325 Total reward: 63.0 Training loss: 1.0711 Explore P: 0.4469\n",
      "Episode: 326 Total reward: 48.0 Training loss: 0.6839 Explore P: 0.4448\n",
      "Episode: 327 Total reward: 34.0 Training loss: 1.2101 Explore P: 0.4433\n",
      "Episode: 328 Total reward: 29.0 Training loss: 39.7453 Explore P: 0.4421\n",
      "Episode: 329 Total reward: 69.0 Training loss: 1.0756 Explore P: 0.4391\n",
      "Episode: 330 Total reward: 50.0 Training loss: 0.9135 Explore P: 0.4370\n",
      "Episode: 331 Total reward: 44.0 Training loss: 1.0305 Explore P: 0.4351\n",
      "Episode: 332 Total reward: 39.0 Training loss: 24.3091 Explore P: 0.4334\n",
      "Episode: 333 Total reward: 26.0 Training loss: 20.5343 Explore P: 0.4323\n",
      "Episode: 334 Total reward: 55.0 Training loss: 24.1132 Explore P: 0.4300\n",
      "Episode: 335 Total reward: 44.0 Training loss: 18.3676 Explore P: 0.4282\n",
      "Episode: 336 Total reward: 29.0 Training loss: 14.8036 Explore P: 0.4270\n",
      "Episode: 337 Total reward: 37.0 Training loss: 1.3016 Explore P: 0.4254\n",
      "Episode: 338 Total reward: 72.0 Training loss: 1.6606 Explore P: 0.4224\n",
      "Episode: 339 Total reward: 56.0 Training loss: 19.9481 Explore P: 0.4201\n",
      "Episode: 340 Total reward: 44.0 Training loss: 1.4216 Explore P: 0.4183\n",
      "Episode: 341 Total reward: 23.0 Training loss: 52.7392 Explore P: 0.4174\n",
      "Episode: 342 Total reward: 30.0 Training loss: 23.8527 Explore P: 0.4162\n",
      "Episode: 343 Total reward: 65.0 Training loss: 1.2234 Explore P: 0.4136\n",
      "Episode: 344 Total reward: 59.0 Training loss: 1.5777 Explore P: 0.4112\n",
      "Episode: 345 Total reward: 25.0 Training loss: 0.7507 Explore P: 0.4102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 346 Total reward: 76.0 Training loss: 26.1816 Explore P: 0.4071\n",
      "Episode: 347 Total reward: 75.0 Training loss: 16.1663 Explore P: 0.4042\n",
      "Episode: 348 Total reward: 51.0 Training loss: 30.1966 Explore P: 0.4022\n",
      "Episode: 349 Total reward: 37.0 Training loss: 21.6012 Explore P: 0.4007\n",
      "Episode: 350 Total reward: 110.0 Training loss: 26.3435 Explore P: 0.3965\n",
      "Episode: 351 Total reward: 43.0 Training loss: 1.0499 Explore P: 0.3948\n",
      "Episode: 352 Total reward: 57.0 Training loss: 114.9817 Explore P: 0.3926\n",
      "Episode: 353 Total reward: 34.0 Training loss: 1.3316 Explore P: 0.3913\n",
      "Episode: 354 Total reward: 35.0 Training loss: 15.7832 Explore P: 0.3900\n",
      "Episode: 355 Total reward: 66.0 Training loss: 24.8487 Explore P: 0.3875\n",
      "Episode: 356 Total reward: 54.0 Training loss: 48.0447 Explore P: 0.3854\n",
      "Episode: 357 Total reward: 45.0 Training loss: 43.8672 Explore P: 0.3838\n",
      "Episode: 358 Total reward: 102.0 Training loss: 1.8264 Explore P: 0.3800\n",
      "Episode: 359 Total reward: 27.0 Training loss: 24.8433 Explore P: 0.3790\n",
      "Episode: 360 Total reward: 73.0 Training loss: 1.1949 Explore P: 0.3763\n",
      "Episode: 361 Total reward: 24.0 Training loss: 100.6477 Explore P: 0.3754\n",
      "Episode: 362 Total reward: 95.0 Training loss: 24.0101 Explore P: 0.3719\n",
      "Episode: 363 Total reward: 43.0 Training loss: 1.9515 Explore P: 0.3704\n",
      "Episode: 364 Total reward: 62.0 Training loss: 1.0701 Explore P: 0.3682\n",
      "Episode: 365 Total reward: 51.0 Training loss: 24.4674 Explore P: 0.3663\n",
      "Episode: 366 Total reward: 45.0 Training loss: 1.4674 Explore P: 0.3647\n",
      "Episode: 367 Total reward: 79.0 Training loss: 3.3232 Explore P: 0.3620\n",
      "Episode: 368 Total reward: 46.0 Training loss: 1.3109 Explore P: 0.3603\n",
      "Episode: 369 Total reward: 64.0 Training loss: 1.8873 Explore P: 0.3581\n",
      "Episode: 370 Total reward: 39.0 Training loss: 1.0674 Explore P: 0.3568\n",
      "Episode: 371 Total reward: 106.0 Training loss: 157.0262 Explore P: 0.3531\n",
      "Episode: 372 Total reward: 50.0 Training loss: 121.3444 Explore P: 0.3514\n",
      "Episode: 373 Total reward: 54.0 Training loss: 1.4068 Explore P: 0.3495\n",
      "Episode: 374 Total reward: 67.0 Training loss: 14.5103 Explore P: 0.3473\n",
      "Episode: 375 Total reward: 46.0 Training loss: 58.3853 Explore P: 0.3457\n",
      "Episode: 376 Total reward: 88.0 Training loss: 3.1465 Explore P: 0.3428\n",
      "Episode: 377 Total reward: 97.0 Training loss: 23.0410 Explore P: 0.3396\n",
      "Episode: 378 Total reward: 49.0 Training loss: 1.4398 Explore P: 0.3380\n",
      "Episode: 379 Total reward: 48.0 Training loss: 28.9442 Explore P: 0.3364\n",
      "Episode: 380 Total reward: 59.0 Training loss: 7.8507 Explore P: 0.3345\n",
      "Episode: 381 Total reward: 54.0 Training loss: 1.0730 Explore P: 0.3327\n",
      "Episode: 382 Total reward: 64.0 Training loss: 2.2884 Explore P: 0.3307\n",
      "Episode: 383 Total reward: 73.0 Training loss: 1.5834 Explore P: 0.3283\n",
      "Episode: 384 Total reward: 26.0 Training loss: 71.9411 Explore P: 0.3275\n",
      "Episode: 385 Total reward: 39.0 Training loss: 27.2440 Explore P: 0.3263\n",
      "Episode: 386 Total reward: 47.0 Training loss: 1.5673 Explore P: 0.3248\n",
      "Episode: 387 Total reward: 49.0 Training loss: 1.7783 Explore P: 0.3233\n",
      "Episode: 388 Total reward: 64.0 Training loss: 1.1769 Explore P: 0.3213\n",
      "Episode: 389 Total reward: 52.0 Training loss: 1.5613 Explore P: 0.3196\n",
      "Episode: 390 Total reward: 83.0 Training loss: 1.9341 Explore P: 0.3171\n",
      "Episode: 391 Total reward: 57.0 Training loss: 24.8070 Explore P: 0.3153\n",
      "Episode: 392 Total reward: 60.0 Training loss: 1.2158 Explore P: 0.3135\n",
      "Episode: 393 Total reward: 91.0 Training loss: 1.9352 Explore P: 0.3108\n",
      "Episode: 394 Total reward: 83.0 Training loss: 59.9871 Explore P: 0.3083\n",
      "Episode: 395 Total reward: 79.0 Training loss: 1.8352 Explore P: 0.3059\n",
      "Episode: 396 Total reward: 40.0 Training loss: 2.2803 Explore P: 0.3047\n",
      "Episode: 397 Total reward: 58.0 Training loss: 1.2654 Explore P: 0.3030\n",
      "Episode: 398 Total reward: 32.0 Training loss: 65.8537 Explore P: 0.3021\n",
      "Episode: 399 Total reward: 31.0 Training loss: 1.2071 Explore P: 0.3012\n",
      "Episode: 400 Total reward: 54.0 Training loss: 1.4454 Explore P: 0.2996\n",
      "Episode: 401 Total reward: 37.0 Training loss: 58.9110 Explore P: 0.2986\n",
      "Episode: 402 Total reward: 31.0 Training loss: 1.3485 Explore P: 0.2977\n",
      "Episode: 403 Total reward: 56.0 Training loss: 2.3052 Explore P: 0.2961\n",
      "Episode: 404 Total reward: 86.0 Training loss: 121.9883 Explore P: 0.2936\n",
      "Episode: 405 Total reward: 73.0 Training loss: 9.9798 Explore P: 0.2915\n",
      "Episode: 406 Total reward: 119.0 Training loss: 2.0016 Explore P: 0.2882\n",
      "Episode: 407 Total reward: 41.0 Training loss: 1.7389 Explore P: 0.2871\n",
      "Episode: 408 Total reward: 39.0 Training loss: 1.9140 Explore P: 0.2860\n",
      "Episode: 409 Total reward: 44.0 Training loss: 43.4174 Explore P: 0.2848\n",
      "Episode: 410 Total reward: 44.0 Training loss: 1.5382 Explore P: 0.2836\n",
      "Episode: 411 Total reward: 82.0 Training loss: 1.7665 Explore P: 0.2813\n",
      "Episode: 412 Total reward: 33.0 Training loss: 41.1015 Explore P: 0.2805\n",
      "Episode: 413 Total reward: 146.0 Training loss: 142.7851 Explore P: 0.2765\n",
      "Episode: 414 Total reward: 31.0 Training loss: 0.9696 Explore P: 0.2757\n",
      "Episode: 415 Total reward: 26.0 Training loss: 108.3855 Explore P: 0.2750\n",
      "Episode: 416 Total reward: 64.0 Training loss: 68.2556 Explore P: 0.2733\n",
      "Episode: 417 Total reward: 145.0 Training loss: 0.8021 Explore P: 0.2695\n",
      "Episode: 418 Total reward: 155.0 Training loss: 1.4252 Explore P: 0.2655\n",
      "Episode: 419 Total reward: 48.0 Training loss: 39.3791 Explore P: 0.2643\n",
      "Episode: 420 Total reward: 45.0 Training loss: 13.4704 Explore P: 0.2632\n",
      "Episode: 421 Total reward: 85.0 Training loss: 36.9042 Explore P: 0.2610\n",
      "Episode: 422 Total reward: 81.0 Training loss: 172.4111 Explore P: 0.2590\n",
      "Episode: 423 Total reward: 87.0 Training loss: 39.0032 Explore P: 0.2569\n",
      "Episode: 424 Total reward: 95.0 Training loss: 2.0744 Explore P: 0.2545\n",
      "Episode: 425 Total reward: 58.0 Training loss: 39.0429 Explore P: 0.2531\n",
      "Episode: 426 Total reward: 30.0 Training loss: 1.0343 Explore P: 0.2524\n",
      "Episode: 427 Total reward: 147.0 Training loss: 2.1959 Explore P: 0.2488\n",
      "Episode: 428 Total reward: 108.0 Training loss: 0.9908 Explore P: 0.2463\n",
      "Episode: 429 Total reward: 84.0 Training loss: 0.9870 Explore P: 0.2443\n",
      "Episode: 430 Total reward: 77.0 Training loss: 2.5331 Explore P: 0.2425\n",
      "Episode: 431 Total reward: 68.0 Training loss: 34.7038 Explore P: 0.2409\n",
      "Episode: 432 Total reward: 49.0 Training loss: 169.0975 Explore P: 0.2398\n",
      "Episode: 433 Total reward: 96.0 Training loss: 132.1444 Explore P: 0.2376\n",
      "Episode: 434 Total reward: 73.0 Training loss: 1.6531 Explore P: 0.2359\n",
      "Episode: 435 Total reward: 62.0 Training loss: 125.2664 Explore P: 0.2346\n",
      "Episode: 436 Total reward: 182.0 Training loss: 1.5647 Explore P: 0.2305\n",
      "Episode: 437 Total reward: 105.0 Training loss: 83.0557 Explore P: 0.2282\n",
      "Episode: 438 Total reward: 63.0 Training loss: 114.2991 Explore P: 0.2268\n",
      "Episode: 439 Total reward: 119.0 Training loss: 1.2714 Explore P: 0.2243\n",
      "Episode: 441 Total reward: 34.0 Training loss: 167.1539 Explore P: 0.2193\n",
      "Episode: 442 Total reward: 169.0 Training loss: 34.9770 Explore P: 0.2158\n",
      "Episode: 443 Total reward: 184.0 Training loss: 1.1371 Explore P: 0.2120\n",
      "Episode: 444 Total reward: 51.0 Training loss: 49.4538 Explore P: 0.2110\n",
      "Episode: 445 Total reward: 40.0 Training loss: 13.4904 Explore P: 0.2102\n",
      "Episode: 446 Total reward: 78.0 Training loss: 182.2804 Explore P: 0.2087\n",
      "Episode: 447 Total reward: 28.0 Training loss: 127.5976 Explore P: 0.2081\n",
      "Episode: 448 Total reward: 51.0 Training loss: 1.1955 Explore P: 0.2071\n",
      "Episode: 449 Total reward: 99.0 Training loss: 1.5163 Explore P: 0.2052\n",
      "Episode: 450 Total reward: 90.0 Training loss: 1.4008 Explore P: 0.2034\n",
      "Episode: 452 Total reward: 19.0 Training loss: 1.5553 Explore P: 0.1992\n",
      "Episode: 453 Total reward: 59.0 Training loss: 131.4750 Explore P: 0.1981\n",
      "Episode: 454 Total reward: 52.0 Training loss: 1.0860 Explore P: 0.1971\n",
      "Episode: 455 Total reward: 50.0 Training loss: 131.7228 Explore P: 0.1962\n",
      "Episode: 456 Total reward: 65.0 Training loss: 0.9800 Explore P: 0.1950\n",
      "Episode: 457 Total reward: 50.0 Training loss: 1.0560 Explore P: 0.1941\n",
      "Episode: 458 Total reward: 200.0 Training loss: 1.3811 Explore P: 0.1904\n",
      "Episode: 459 Total reward: 84.0 Training loss: 1.2264 Explore P: 0.1889\n",
      "Episode: 460 Total reward: 89.0 Training loss: 0.7649 Explore P: 0.1873\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 461 Total reward: 186.0 Training loss: 1.1491 Explore P: 0.1841\n",
      "Episode: 462 Total reward: 150.0 Training loss: 51.6128 Explore P: 0.1815\n",
      "Episode: 463 Total reward: 95.0 Training loss: 0.8115 Explore P: 0.1798\n",
      "Episode: 464 Total reward: 76.0 Training loss: 46.0825 Explore P: 0.1786\n",
      "Episode: 465 Total reward: 111.0 Training loss: 1.4964 Explore P: 0.1767\n",
      "Episode: 466 Total reward: 123.0 Training loss: 1.2264 Explore P: 0.1747\n",
      "Episode: 468 Total reward: 20.0 Training loss: 1.1146 Explore P: 0.1711\n",
      "Episode: 469 Total reward: 190.0 Training loss: 1.2235 Explore P: 0.1680\n",
      "Episode: 470 Total reward: 133.0 Training loss: 1.4084 Explore P: 0.1660\n",
      "Episode: 472 Total reward: 67.0 Training loss: 1.1552 Explore P: 0.1619\n",
      "Episode: 473 Total reward: 122.0 Training loss: 1.1553 Explore P: 0.1600\n"
     ]
    }
   ],
   "source": [
    "# Now train with experiences\n",
    "saver = tf.train.Saver()\n",
    "rewards_list = []\n",
    "with tf.Session() as sess:\n",
    "    # Initialize variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    step = 0\n",
    "    for ep in range(1, train_episodes):\n",
    "        total_reward = 0\n",
    "        t = 0\n",
    "        while t < max_steps:\n",
    "            step += 1\n",
    "            # Uncomment this next line to watch the training\n",
    "            # env.render() \n",
    "            \n",
    "            # Explore or Exploit\n",
    "            explore_p = explore_stop + (explore_start - explore_stop)*np.exp(-decay_rate*step) \n",
    "            if explore_p > np.random.rand():\n",
    "                # Make a random action\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                # Get action from Q-network\n",
    "                feed = {mainQN.inputs_: state.reshape((1, *state.shape))}\n",
    "                Qs = sess.run(mainQN.output, feed_dict=feed)\n",
    "                action = np.argmax(Qs)\n",
    "            \n",
    "            # Take action, get new state and reward\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "    \n",
    "            total_reward += reward\n",
    "            \n",
    "            if done:\n",
    "                # the episode ends so no next state\n",
    "                next_state = np.zeros(state.shape)\n",
    "                t = max_steps\n",
    "                \n",
    "                print('Episode: {}'.format(ep),\n",
    "                      'Total reward: {}'.format(total_reward),\n",
    "                      'Training loss: {:.4f}'.format(loss),\n",
    "                      'Explore P: {:.4f}'.format(explore_p))\n",
    "                rewards_list.append((ep, total_reward))\n",
    "                \n",
    "                # Add experience to memory\n",
    "                memory.add((state, action, reward, next_state))\n",
    "                \n",
    "                # Start new episode\n",
    "                env.reset()\n",
    "                # Take one random step to get the pole and cart moving\n",
    "                state, reward, done, _ = env.step(env.action_space.sample())\n",
    "\n",
    "            else:\n",
    "                # Add experience to memory\n",
    "                memory.add((state, action, reward, next_state))\n",
    "                state = next_state\n",
    "                t += 1\n",
    "            \n",
    "            # Sample mini-batch from memory\n",
    "            batch = memory.sample(batch_size)\n",
    "            states = np.array([each[0] for each in batch])\n",
    "            actions = np.array([each[1] for each in batch])\n",
    "            rewards = np.array([each[2] for each in batch])\n",
    "            next_states = np.array([each[3] for each in batch])\n",
    "            \n",
    "            # Train network\n",
    "            target_Qs = sess.run(mainQN.output, feed_dict={mainQN.inputs_: next_states})\n",
    "            \n",
    "            # Set target_Qs to 0 for states where episode ends\n",
    "            episode_ends = (next_states == np.zeros(states[0].shape)).all(axis=1)\n",
    "            target_Qs[episode_ends] = (0, 0)\n",
    "            \n",
    "            targets = rewards + gamma * np.max(target_Qs, axis=1)\n",
    "\n",
    "            loss, _ = sess.run([mainQN.loss, mainQN.opt],\n",
    "                                feed_dict={mainQN.inputs_: states,\n",
    "                                           mainQN.targetQs_: targets,\n",
    "                                           mainQN.actions_: actions})\n",
    "        \n",
    "    saver.save(sess, \"checkpoints/cartpole.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing training\n",
    "\n",
    "Below we plot the total rewards for each episode. The rolling average is plotted in blue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def running_mean(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / N "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eps, rews = np.array(rewards_list).T\n",
    "smoothed_rews = running_mean(rews, 10)\n",
    "plt.plot(eps[-len(smoothed_rews):], smoothed_rews)\n",
    "plt.plot(eps, rews, color='grey', alpha=0.3)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![png](output_21_1.png)\n",
    "\n",
    "\n",
    "## Playing Atari Games\n",
    "\n",
    "So, Cart-Pole is a pretty simple game. However, the same model can be used to train an agent to play something much more complicated like Pong or Space Invaders. Instead of a state like we're using here though, you'd want to use convolutional layers to get the state from the screen images.\n",
    "\n",
    "![Deep Q-Learning Atari](assets/atari-network.png)\n",
    "\n",
    "I'll leave it as a challenge for you to use deep Q-learning to train an agent to play Atari games. Here's the original paper which will get you started: http://www.davidqiu.com:8888/research/nature14236.pdf."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
